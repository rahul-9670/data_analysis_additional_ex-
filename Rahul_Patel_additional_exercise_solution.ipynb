{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Additional Exercise\n",
    "\n",
    "Due: **12:00 18 July 2022**\n",
    "\n",
    "**Online submission** at via [ILIAS](https://www.ilias.uni-koeln.de/ilias/goto_uk_exc_4593683.html) in the directory Exercises / Übungen -> Submission of Exercises / Rückgabe des Übungsblätter\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Information"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This additional exercise may be used for **one** of two reasons:\n",
    "\n",
    "* make up for the attendance requirement (if you missed more than two tutorials), or\n",
    "* replace lowest grade in exercises (mentioned to you via email)\n",
    "\n",
    "For the latter, you may submit the solutions to this like any other exercise this year. For experience, it is advantageous to still use the following procedure.\n",
    "\n",
    "If you need to make up for the attendance requirement, you must use a specific method of submission utilising [github](https://github.com/) and a makefile. If you are using Windows, please ensure you work either in [cygwin](https://www.cygwin.com/) or [Windows Subsystem for Linux](https://docs.microsoft.com/en-us/windows/wsl/install), since `make` does not work in Windows.\n",
    "\n",
    "1. create a repository on github exclusively for this exercise\n",
    "2. fully complete (or at least attempt) each question in a jupyter notebook\n",
    "   * name it `[name]_additional_exercise_solution.ipynb`\n",
    "3. restart and clear all notebook ouputs so it contains just the markdown and code cells (no plots or other outputs)\n",
    "4. create a makefile to execute the notebook, compile to a pdf, and remove any temporary files\n",
    "   * I should only need to run `$make` in order to build your submission\n",
    "   * **do not** submit a compiled pdf of the solution\n",
    "5. submit a file `[name].txt` containing the link to your git repository\n",
    "\n",
    "Although make is a rather archaic tool, `make` is very useful to perform low-level compilation. It actually began as an easier method to compile C** and fortran code. Common compilation routines are identified using *suffix rules*. **For an additional 10%, write your own suffix rule to compile `.ipynb` files into `.pdf`.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `MakeFile`s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What a makefile does is simplify a workflow using terminal commands. As mentioned above, it is useful to compile documentation/code if changes where made in the source. One can in fact use this for newer formats such as IPython notebooks (`.ipynb` files).\n",
    "\n",
    "A makefile typically goes in the top directory of your project. You can find an introduction to makefiles [here](https://makefiletutorial.com/). Basically how a makefile works is that you have a series of *targets* that execute a specific set of commands. For example, if your makefile contains,\n",
    "\n",
    "```\n",
    "info:\n",
    "    @echo \"This is a makefile.\"\n",
    "\n",
    "command:\n",
    "    @echo \"This is a command.\"\n",
    "```\n",
    "\n",
    "> *Note:* Make will by default also print each command. Beginning the command with `@` suppresses this functionality.\n",
    "\n",
    "you can make either target `info` or target `command`. Running `make info` in the terminal from the top directory will print `This is a makefile.` to the terminal. Likewise running `make command` will print `This is a command.` to the terminal. Not specifying a target will make the first target in the file (so `make info` is the same as `make` in this example). For this reason, it is customary to set the first target as the default workflow. If our makefile now contains,\n",
    "\n",
    "```\n",
    "all: info command\n",
    "\n",
    "info:\n",
    "    @echo \"This is a makefile.\"\n",
    "\n",
    "command:\n",
    "    @echo \"Some commands to execute:\"\n",
    "    echo \"...\"\n",
    "    @echo \"It has now completed.\"\n",
    "```\n",
    "\n",
    "running `make` from the terminal will print,\n",
    "\n",
    "```\n",
    "This is a makefile.\n",
    "Some commands to execute:\n",
    "echo \"...\"\n",
    "...\n",
    "It has now completed.\n",
    "```\n",
    "\n",
    "Each target can run multiple commands, but each line is run in a separate subshell. This means if you want to string together commands (for example if you need to change directories), you have to write them in one line as `command 1 ; command 2 ; ...`.\n",
    "\n",
    "This should be most everything you need to write your own makefile. Please contact a TA if you run into any major issues.\n",
    "\n",
    "> Ensure each command is indented with a **tab** character (not *space* characters). This is one of the finicky aspects of using a makefile...\n",
    "\n",
    "> Note that linux operating systems use GNU make with `bash` or `dash` as a shell. Macintosh products typically use `zsh`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Working with a notebook from the terminal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most operations you can perform on the notebook are available from the terminal. The command to work with notebooks is [nbconvert](https://nbconvert.readthedocs.io/en/latest/usage.html). This can convert between file types, depending on your needs. For example, executing a notebook from the terminal can be done using,\n",
    "\n",
    "```\n",
    "jupyter nbconvert --to notebook --execute notebook.ipynb\n",
    "```\n",
    "\n",
    "You can see a list of the available format types in the aforementioned link."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## the *Hipparcos* catalogue\n",
    "\n",
    "*Hipparcos* (HIgh Precision PARallax COllecting Satelite) was a very important stellar survey mission between 1989 and 1993. It was the first astrometric mission of its kind, and it's precise observations allowed for unprecedented astrometrical calculations within the Milky Way. The observations of faint stars have since been (*vastly*) improved upon by the current *Gaia* mission, though the brightest stars were still most accurately observed by *Hipparcos*.\n",
    "\n",
    "The following examples will utilise a subset of a more recent reduction of the *Hipparcos* [data](https://vizier.u-strasbg.fr/viz-bin/VizieR-3?-source=I/311/hip2) ([van Leeuwen 2007](https://arxiv.org/pdf/0708.1752.pdf)) to emphasize how work with astronomical data within python, and hopefully teach a thing or two about stellar populations. The dataset we use are the stars with five-parameter astrometric solutions, meaning the solutions have been constrained for RA and DEC, proper motion in RA and DEC, and parallax. We will also highlight some of the features of various python packages so you can decide which method you prefer.\n",
    "\n",
    "This dataset has 15 columns, with the titles as a comment in the first line. The columns (and units) are,\n",
    "\n",
    " - name in the *Hipparcos* catalog HIP\n",
    " - right ascentions RA ($\\mathrm{RA}$; degrees)\n",
    " - error in right ascention sigma_RA ($\\sigma_\\mathrm{RA}$; milli-arcseconds)\n",
    " - declination DEC ($\\mathrm{DEC}$; degrees)\n",
    " - error in declination sigma_DEC ($\\sigma_\\mathrm{DEC}$; milli-arcseconds)\n",
    " - parallax PLX ($\\varpi$; in milli-arcseconds)\n",
    " - error in parallax sigma_PLX ($\\sigma_\\mathrm{\\varpi}$; in milli-arcseconds)\n",
    " - right ascention proper motion PM_RA ($\\mu_\\mathrm{RA}$; milli-arcseconds per year)\n",
    " - error in right ascention proper motion sigma_PM_RA ($\\sigma_{\\mu, \\mathrm{RA}}$; milli-arcseconds per year)\n",
    " - declination proper motion PM_DEC ($\\mu_\\mathrm{DEC}$; milli-arcseconds per year)\n",
    " - error in declination proper motion sigma_PM_DEC ($\\sigma_{\\mu, \\mathrm{DEC}}$; milli-arcseconds per year)\n",
    " - *Hipparcos* magnitude Hp (dex)\n",
    "   - *aka* V-band magnitude\n",
    " - error in magnitude sigma_Hp (dex)\n",
    " - B-V color (dex)\n",
    " - error in B-V color sigma_B-V (dex)\n",
    " - V-I color (dex)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculating distance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A major benefit to precise astrometric observations is an accurate estimate of the parallax. This allows us to calculate the distance to each star and it reveal the structure of the Milky Way.\n",
    "\n",
    "Begin by using the parallax ($\\varpi$) to calculate the distance in parsecs (pc) to each star. It is important to remember that the parallax has units of milli-arcseconds (mas), so the distance to the star is calculated by:\n",
    "\n",
    "$$ d = 1000 \\Big(\\frac{\\mathrm{mas}}{\\varpi}\\Big) \\ \\mathrm{pc} $$\n",
    "\n",
    "Use this formula to find some interesting trends in the *Hipparcos* catalog. Are there any issues with the dataset? **5 points**\n",
    "\n",
    "> *Note:* Each degree contains 60 arcminutes, which each contain 60 arcseconds, so there are 3 600 000 milli-arcseconds in 1 degree.\n",
    "\n",
    "> *Note:* The Milky Way has a diameter of $\\sim 40~000$ pc and Earth is located at a radius of $\\sim 8~000$ pc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import necessary packages\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "pd.options.mode.chained_assignment = None  # default='warn'\n",
    "\n",
    "# load the data in the given files\n",
    "data=pd.read_csv('hipparcos2.csv') \n",
    "data['PLX'][data['PLX']==0]=np.nan # remove zero parallax\n",
    "data['d']=1000*(1/data['PLX']) # calculate the distance from the parallaxes\n",
    "print('d:',data['d'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are negative distances in the dataset because of the negative values of parallax.\n",
    "Then there are distances whose values are infinite!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Can you find what is the most common distance to a star? What is the standard deviation of this value? What can you determine from these values? **5 points**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d=np.array(data['d'])\n",
    "values, counts = np.unique(d, return_counts=True) # counting all the values and their frequency\n",
    "ind = np.argmax(counts) # index for the value that appears the most\n",
    "print('The most common distance to a star is:',values[ind],'pc') # most common distance\n",
    "mean=np.nanmean(d) # mean of distances\n",
    "std=np.abs(mean-values[ind]) # standard deviation of the most common value\n",
    "print('The standard deviation of the most common value from the mean distance is:', std,'pc')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Utilising error\n",
    "\n",
    "Error is a major aspect of observational data, since there are so many sources (instrumentational and physical) to consider. Following the procedure of van Leeuwen (2007), let us examine the error in parallax and its trends in our dataset. Relative error $\\big(\\frac{\\sigma_q}{q}\\big)$ is an important metric when our data spans multiple orders of magnitude. What is the distribution of relative error in parallax (you may want to make y log-spaced)? What does this mean in terms of *Hipparcos*' precision? **10 points**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculating the relative error in parallaxes\n",
    "data['rel_err_PLX']=data['sigma_PLX']/data['PLX']\n",
    "#plotting the distribution of relative errors \n",
    "plt.hist(data['rel_err_PLX'],ec='black')\n",
    "plt.yscale('log') # making y log-spaced\n",
    "plt.title('distribution of relative error in parallax')\n",
    "plt.xlabel('relative error');plt.ylabel('counts(log)');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The error in parallax is typically related to the brightness of the star (from the statistics of photon counts). A view of this trend can be seen below (keeping the relative parallax error below 5%; similar to Figure 11 in van Leewen 2007). Can you find a fit to this relation? **10 points**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![relative error](hipparcos_plx_err.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotting parallax error for relative error below 5%\n",
    "from scipy.optimize import curve_fit\n",
    "err_plx_5=data['sigma_PLX'][data['rel_err_PLX']<0.05]\n",
    "plt.scatter(data['Hp'][data['rel_err_PLX']<0.05],err_plx_5,s=1)\n",
    "plt.gca().invert_xaxis()\n",
    "# fit function\n",
    "def func(x, a, b):\n",
    "    return a*x + b\n",
    "# fit parameters\n",
    "params = curve_fit(func, data['Hp'][data['rel_err_PLX']<0.05], err_plx_5)\n",
    "a=params[0][0]\n",
    "b=params[0][1]\n",
    "# y values for the fit line\n",
    "y=func(data['Hp'][data['rel_err_PLX']<0.05],a,b)\n",
    "# plotting the fit on the data\n",
    "plt.plot(data['Hp'][data['rel_err_PLX']<0.05],y,c='red')\n",
    "plt.ylim(-10,40)\n",
    "plt.title('fit to parallax error for relative error below 5%')\n",
    "plt.xlabel('Hp');plt.ylabel('sigma_PLX')\n",
    "plt.legend(['sigma_PLX','fit line']);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Can you plot a histogram of the relative errors in $\\mathrm{RA}, \\ \\mathrm{DEC},$ and $\\mathrm{PLX}$ at the same time? **10 points**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "err_RA=data['sigma_RA']/data['RA'] # relative error in RA\n",
    "err_DEC=data['sigma_DEC']/data['DEC'] # # relative error in DEC\n",
    "err_PLX=data['rel_err_PLX'] # relative error in PLX\n",
    "# plotting the histogram of relative errors in RA, DEC, PLX\n",
    "plt.hist([err_RA, err_DEC, err_PLX], label=['err_RA', 'err_DEC','err_PLX'])\n",
    "plt.yscale('log')\n",
    "plt.title('histogram of relative errors in RA, DEC, PLX')\n",
    "plt.xlabel('relative errors'); plt.ylabel('counts')\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hertzsprung-Russell diagrams\n",
    "\n",
    "#### Observational\n",
    "\n",
    "Before calculating any intrinsic stellar properties from the observations, it is typically useful to plot a diagram of what you *observe*. (Observed here means the most basic properties that can be determined from the data. If you want to be pedantic, then you might suggest that the values in this table are not what is actually observed, but a reduction of the observed photon counts.) For this purpose we should plot a Hertzsprung-Russel diagram (HRD) to reveal the different stellar types. It is important not only to remember that a HRD has colour (B-V) on the x-axis and magnitude (Hp) on the y-axis, but also that the y-axis is reversed (the perks of using an antiquated unit). **5 points**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotting a Hertzsprung-Russel diagram (HRD)\n",
    "plt.gca().invert_yaxis() # revert y axis\n",
    "plt.scatter(data['B-V'],data['Hp'],s=2)\n",
    "plt.title('HRD')\n",
    "plt.xlabel('B-V'); plt.ylabel('Hp');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What do you see?\n",
    "\n",
    "If you use the magnitude given in `hipparcos2.csv`, you might have a little difficulty determining the different regions of an HR diagram. One reason for this is that it is an apparent magnitude, so these results do not take into account reddening or extinction. Another issue, however, is that we don't filter the results that have a lot of error. Try plotting it again while limiting the relative parallax error to <20% and the color error to 5 mag. **5 points**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# HRD with relative error < 20% and color error < 5 mag\n",
    "x=data['B-V'][(err_PLX<0.2) & (data['sigma_B-V']<0.05)]\n",
    "y=data['Hp'][(err_PLX<0.2) & (data['sigma_B-V']<0.05)]\n",
    "plt.gca().invert_yaxis()\n",
    "plt.scatter(x,y,s=2)\n",
    "plt.title('HRD with relative error < 20% and color error < 5 mag')\n",
    "plt.xlabel('B-V'); plt.ylabel('Hp');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One trait now looks more prominent, and that is the suspiciously high number of stars with $ B-V = 0 $. This is called an artefact, and it is a result of some bad data or sloppy reduction. Since the author of this reduction is quite renowned in the community, the issue most likely lies with bad data for these stars. For the rest of the exercise, you may filter-out star with $B-V = 0$.\n",
    "\n",
    "Now try plotting the HRD in terms of the intrinsic absolute magnitude, $ M_{Hp} $ (you should get something similar in shape to [this](https://www.cosmos.esa.int/documents/532822/573165/f3_5_005.pdf/90951100-6586-4ba7-a042-784f5845a279)). **Try also with the other colour $V-I$**. Remember that the absolute magnitude can be calculated by the formula:\n",
    "\n",
    "$$\n",
    "M_\\mathrm{Hp} = m_\\mathrm{Hp} - 5 \\ \\mathrm{log}_\\mathrm{10}\\Big(\\frac{d}{pc}\\Big) + 5\n",
    "$$\n",
    "\n",
    "This plot has a bit of an issue where there are many points plotted ontop of each other. Instead, see if you can plot the number density in a 2D histogram of of the following ways **10 points**\n",
    "\n",
    "* bin the data in steps of 0.05 mag in $M_{Hp}$ and 0.01 mag in $B-V$, with the number density in the color bar. You might find the function `scipy.stats.binned_statistic_2d()` useful.\n",
    "* Utilise the `hexbin` routine with an adequate choice of binning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "x=np.array(x)\n",
    "y=np.array(y)\n",
    "index=x==0 # get index of all B-V values equal to zero \n",
    "dist=d[(err_PLX<0.2) & (data['sigma_B-V']<0.05)]\n",
    "x_new=np.delete(x,np.where(x==0)) # remove values corresponding to B-V = 0 values from x\n",
    "y_new=np.delete(y,np.where(x==0)) # remove values corresponding to B-V = 0 values from y\n",
    "d_new=np.delete(dist,np.where(x==0)) # remove B-V = 0 values from d\n",
    "M=y_new-5*np.log10(np.abs(d_new))+5 # M_Hp\n",
    "plt.gca().invert_yaxis()\n",
    "plt.scatter(x_new,M,s=2)\n",
    "plt.title('HRD in terms of the intrinsic absolute magnitude')\n",
    "plt.xlabel('B-V');plt.ylabel('M_Hp');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Very broadly we say that the features observed here are the main sequence (diagonal feature) and red clump (roughly spherical feature). Fit a line to the main sequence. Approximate the percentage of stars in the main sequence compared to the percentage in the red clump. **10 points**\n",
    "\n",
    "> *Note:* This will not add up to 100%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Theoretical\n",
    "\n",
    "For theorists it is more illuminating to plot diagrams of intrinsic properties. This makes it easier to relate the observations to population synthesis or evolutionary models. Getting to these intrinsic stellar properties from our observations is typically quite involved, requiring some (not so trivial) estimate of the dust extinction and attenuation along the line of sight. We will assume there is no dust extinction for a very basic example.\n",
    "\n",
    "A star's colour can be used to calculate its effective (surface) temperature. Disregarding any reddenning, we can estimate it using the formula ([Ballesteros 2012](https://arxiv.org/pdf/1201.1809.pdf)):\n",
    "\n",
    "$$\n",
    "T_\\mathrm{eff} = 4600 \\ \\Big( \\frac{1}{0.92(B-V)+0.62} + \\frac{1}{0.92(B-V)+1.7} \\Big) \\ \\mathrm{K}\n",
    "$$\n",
    "\n",
    "Luminosity is the intrinsic analog of magnitude. To put it in solar units, we need to use the absolute bolometric magnitude of the Sun ($M_\\odot=4.74$) with the observed star's bolometric magnitude and extinction. Since our absolute magnitudes are in the V band, we will have to add a bolometric correction ($BC_\\mathrm{Hp}$) to convert to the bolometric magnitude. Thus we have too many unknowns for our to make an accurate approximation. Current missions (ie. *Gaia*) get around this by utilising extremely-randomised trees, a regression method beyond the scope of this simple introduction. We will **not** use regression, but instead use an extremely-simplified approach where we neglect extinction and interpolate the bolometric correction (from `BC_hipparcos.csv`; [Masana et al. 2008](https://arxiv.org/pdf/astro-ph/0601049.pdf)) to get the formula:\n",
    "\n",
    "$$\n",
    "-2.5 \\ \\mathrm{log_{10}} \\Big( \\frac{L}{L_\\odot} \\Big) = M_\\mathrm{Hp} + BC_\\mathrm{Hp}(T_\\mathrm{eff}) - M_\\odot\n",
    "$$\n",
    "\n",
    "Finally, the stellar radii can be calculated using the standard blackbody relation:\n",
    "\n",
    "$$\n",
    "L = \\Big( \\frac{R}{R_\\odot} \\Big)^2 \\Big( \\frac{T_\\mathrm{eff}}{T_{\\mathrm{eff},\\odot}} \\Big)^4 \\ L_\\odot\n",
    "$$\n",
    "\n",
    "It might also help you to know the solar units: $L_\\odot=3.828*10^{26} \\ \\mathrm{W}$, $R_\\odot=6.956*10^8 \\ \\mathrm{m}$, $T_\\odot=5780 \\ \\mathrm{K}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perform a spline fit of the bolometric correction data. Correctly interpolate the BC and utilise the equations to create a 2D histogram (or hexbin plot) of $L$ as a function of $T_\\mathrm{eff}$. **10 points**\n",
    "\n",
    "> *hint:* Use units of $L_\\odot$ and K.\n",
    "\n",
    "> *hint:* Technically this fit is only valid for $T_\\mathrm{eff} \\in [3300, 8000]$ K."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now try to perform a linear fit to the main sequence, accounting for error. **10 points**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Figure out how to isolate the stars on the main sequence. What is the probability for a star to be on the main sequence? Use the bootstrapping technique to determine the constrain the error in this approximation. **10 points**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
